{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projections LHS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHYJUygL40yK1xu3je2Ao4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMpDUxAqfS0Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in Data\n",
        "\n",
        "\n",
        "* Must have the 'Ridership Model Data.csv' in the project folder;\n",
        "* Create model_data for training the model which has non-relevant features dropped; and\n",
        "* Create features to be used for generating future feature values.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZghNcTXtA1bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in data and split in to training and prediction sets\n",
        "\n",
        "df = pd.read_csv('Ridership Model Data.csv')\n",
        "model_data = df.drop(['ISOWeek','Month','Start of Week','Population Growth Rate',\n",
        "                      'critical cases','BC Vaccination Rate'], axis=1)\n",
        "features = model_data.drop(['Total Boardings'], axis=1)"
      ],
      "metadata": {
        "id": "vlS7UrKufW1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to get Test Data\n",
        "\n",
        "* Choose a year to generate values for;\n",
        "* Select whether the data should reflect pre- or post-covid scenarios; and\n",
        "* Select whether the data should include 20,000 hours of expansion.\n",
        "\n",
        "## Methods\n",
        "\n",
        "* Data is generally generated using a normal distribution with a mean and standard deviation taken from the true data set;\n",
        "* Some exceptions are the:\n",
        "  * Year which is the specified year; and\n",
        "  * Restaurant Bookings which follows a logarithmic distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "OjIGeGXBBgFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rb_coefs = np.polyfit(np.log(features[features['hospitalizations'] > 0].index.values),\n",
        "                      features[features['hospitalizations'] > 0]['Restaurant Bookings'],1)\n",
        "\n",
        "def get_test_data(year,pre_covid=False,expansion=True):\n",
        "  '''\n",
        "  Generate randomized data for a specific year.\n",
        "  '''\n",
        "  if year > 2021:\n",
        "    inc = year - 2021\n",
        "  else:\n",
        "    inc = 0\n",
        "\n",
        "  if pre_covid:\n",
        "    year = [2019] * 52\n",
        "  else:\n",
        "    year = [2021] * 52\n",
        "  if expansion:\n",
        "    revenue_hours = np.random.normal(features.describe()['Revenue Hours']['mean'] + inc * 20000 / 52,\n",
        "                                     features.describe()['Revenue Hours']['std'], size=52)\n",
        "  else:\n",
        "    revenue_hours = np.random.normal(features.describe()['Revenue Hours']['mean'],\n",
        "                                     features.describe()['Revenue Hours']['std'], size=52)\n",
        "  if pre_covid:\n",
        "    restaurant_bookings = np.random.normal(0,\n",
        "                                           features[features['hospitalizations'] > 0].describe()['Restaurant Bookings']['std'],\n",
        "                                           size=52)\n",
        "  else:\n",
        "    restaurant_bookings = np.random.normal(rb_coefs[0] * np.log(range(104 + inc * 52,156 + inc * 52)) + rb_coefs[1],\n",
        "                                           features.describe()['Restaurant Bookings']['std'])\n",
        "  gas_price = np.random.normal(features.describe()['Gas Price (C/L)']['mean'],\n",
        "                               features.describe()['Gas Price (C/L)']['std'],size=52)\n",
        "  if np.random.randn() > 0 or pre_covid:\n",
        "    uni_season = features[features['ISOYear'] == 2019]['University School Season']\n",
        "  else:\n",
        "    uni_season = features[features['ISOYear'] == 2021]['University School Season']\n",
        "  employment = np.random.normal(features.describe()['Employment']['mean'] * (1 + inc * .01),\n",
        "                                features.describe()['Employment']['std'],size=52)\n",
        "  if pre_covid:\n",
        "    wfh = np.random.normal(features[features['ISOYear'] == 2019].describe()['WFH']['mean'],\n",
        "                           features[features['ISOYear'] == 2019].describe()['WFH']['std'],size=52)\n",
        "  else:\n",
        "    wfh = np.random.normal(features.describe()['WFH']['mean'],features.describe()['WFH']['std'],size=52)\n",
        "  if pre_covid:\n",
        "    hospitalizations = np.zeros(52)\n",
        "  else:\n",
        "    hospitalizations = np.random.normal(features[features['hospitalizations'] > 0].describe()['hospitalizations']['mean'],\n",
        "                                        features[features['hospitalizations'] > 0].describe()['hospitalizations']['std'],\n",
        "                                        size=52)\n",
        "\n",
        "  df = np.array([year, revenue_hours, restaurant_bookings, gas_price, uni_season, employment, wfh, hospitalizations]).transpose()\n",
        "  df = pd.DataFrame(data=df,columns=features.columns)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "mPCv6zabfvWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Test Split"
      ],
      "metadata": {
        "id": "MQSDCME1Cf2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "J2mjZtuWhD5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data for model training.\n",
        "X = model_data.drop('Total Boardings',axis=1)\n",
        "y = model_data['Total Boardings']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "l-UviZbKhHzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create and Train the Model\n",
        "\n",
        "* Hyperparameters have been selected based on previous testing of the model."
      ],
      "metadata": {
        "id": "kZvS22IuCihI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "22mfGQQ8hZ5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep fully-connected neural network consisting of four layers (8 -> 5 -> 3 -> 1).\n",
        "# Rectified linear unit activation functions for non-ouput layers.\n",
        "# No dropouts as this hindered performance in the model testing.\n",
        "# Adam optimizer and mse loss function for regression on a single output variable.\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(5,activation='relu'))\n",
        "model.add(Dense(3,activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='Adam', loss='mse')"
      ],
      "metadata": {
        "id": "wNnv407AhgvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to all our data - 200 epochs chosen based on test runs with validation data.\n",
        "model.fit(x=X,y=y,batch_size=1,epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77RRAY1zhkJJ",
        "outputId": "dc5a14c4-92c8-4a3a-c4f3-5e1d0466e25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 210358943744.0000\n",
            "Epoch 2/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 210355666944.0000\n",
            "Epoch 3/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 210333696000.0000\n",
            "Epoch 4/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 210240503808.0000\n",
            "Epoch 5/200\n",
            "157/157 [==============================] - 0s 1ms/step - loss: 209957470208.0000\n",
            "Epoch 6/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 209354997760.0000\n",
            "Epoch 7/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 208284254208.0000\n",
            "Epoch 8/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 206613037056.0000\n",
            "Epoch 9/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 204191105024.0000\n",
            "Epoch 10/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 200916238336.0000\n",
            "Epoch 11/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 196669833216.0000\n",
            "Epoch 12/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 191397576704.0000\n",
            "Epoch 13/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 185060933632.0000\n",
            "Epoch 14/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 177660411904.0000\n",
            "Epoch 15/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 169248522240.0000\n",
            "Epoch 16/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 159893831680.0000\n",
            "Epoch 17/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 149768945664.0000\n",
            "Epoch 18/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 139006132224.0000\n",
            "Epoch 19/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 127849078784.0000\n",
            "Epoch 20/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 116494753792.0000\n",
            "Epoch 21/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 105207701504.0000\n",
            "Epoch 22/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 94276329472.0000\n",
            "Epoch 23/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 83977338880.0000\n",
            "Epoch 24/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 74501758976.0000\n",
            "Epoch 25/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 65995571200.0000\n",
            "Epoch 26/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 58541522944.0000\n",
            "Epoch 27/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 52192260096.0000\n",
            "Epoch 28/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 46957686784.0000\n",
            "Epoch 29/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 42703339520.0000\n",
            "Epoch 30/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 39355199488.0000\n",
            "Epoch 31/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 36684025856.0000\n",
            "Epoch 32/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 34609909760.0000\n",
            "Epoch 33/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 32954238976.0000\n",
            "Epoch 34/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 31638149120.0000\n",
            "Epoch 35/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 30553376768.0000\n",
            "Epoch 36/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 29669787648.0000\n",
            "Epoch 37/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 28910325760.0000\n",
            "Epoch 38/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 28239988736.0000\n",
            "Epoch 39/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 27637598208.0000\n",
            "Epoch 40/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 27113396224.0000\n",
            "Epoch 41/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 26600398848.0000\n",
            "Epoch 42/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 26118340608.0000\n",
            "Epoch 43/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 25663250432.0000\n",
            "Epoch 44/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 25227921408.0000\n",
            "Epoch 45/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 24810680320.0000\n",
            "Epoch 46/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 24405878784.0000\n",
            "Epoch 47/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 24013404160.0000\n",
            "Epoch 48/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 23619016704.0000\n",
            "Epoch 49/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 23249389568.0000\n",
            "Epoch 50/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 22863001600.0000\n",
            "Epoch 51/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 22507948032.0000\n",
            "Epoch 52/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 22147356672.0000\n",
            "Epoch 53/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 21786621952.0000\n",
            "Epoch 54/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 21447157760.0000\n",
            "Epoch 55/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 21098391552.0000\n",
            "Epoch 56/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 20762025984.0000\n",
            "Epoch 57/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 20422584320.0000\n",
            "Epoch 58/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 20090611712.0000\n",
            "Epoch 59/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 19782656000.0000\n",
            "Epoch 60/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 19442157568.0000\n",
            "Epoch 61/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 19171639296.0000\n",
            "Epoch 62/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 18820415488.0000\n",
            "Epoch 63/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 18498562048.0000\n",
            "Epoch 64/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 18192693248.0000\n",
            "Epoch 65/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 17909508096.0000\n",
            "Epoch 66/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 17595922432.0000\n",
            "Epoch 67/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 17301047296.0000\n",
            "Epoch 68/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 17009587200.0000\n",
            "Epoch 69/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 16746635264.0000\n",
            "Epoch 70/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 16447210496.0000\n",
            "Epoch 71/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 16148800512.0000\n",
            "Epoch 72/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 15903894528.0000\n",
            "Epoch 73/200\n",
            "157/157 [==============================] - 0s 1ms/step - loss: 15626238976.0000\n",
            "Epoch 74/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 15331246080.0000\n",
            "Epoch 75/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 15064174592.0000\n",
            "Epoch 76/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 14799641600.0000\n",
            "Epoch 77/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 14536133632.0000\n",
            "Epoch 78/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 14290115584.0000\n",
            "Epoch 79/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 14027555840.0000\n",
            "Epoch 80/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 13772456960.0000\n",
            "Epoch 81/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 13530258432.0000\n",
            "Epoch 82/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 13291186176.0000\n",
            "Epoch 83/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 13057586176.0000\n",
            "Epoch 84/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 12810010624.0000\n",
            "Epoch 85/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 12580592640.0000\n",
            "Epoch 86/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 12355472384.0000\n",
            "Epoch 87/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 12132584448.0000\n",
            "Epoch 88/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 11915034624.0000\n",
            "Epoch 89/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 11692672000.0000\n",
            "Epoch 90/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 11486188544.0000\n",
            "Epoch 91/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 11275681792.0000\n",
            "Epoch 92/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 11068027904.0000\n",
            "Epoch 93/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 10884018176.0000\n",
            "Epoch 94/200\n",
            "157/157 [==============================] - 0s 1ms/step - loss: 10676826112.0000\n",
            "Epoch 95/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 10491579392.0000\n",
            "Epoch 96/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 10298304512.0000\n",
            "Epoch 97/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 10123778048.0000\n",
            "Epoch 98/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 9928015872.0000\n",
            "Epoch 99/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 9762099200.0000\n",
            "Epoch 100/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 9589884928.0000\n",
            "Epoch 101/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 9418307584.0000\n",
            "Epoch 102/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 9271043072.0000\n",
            "Epoch 103/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 9098846208.0000\n",
            "Epoch 104/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 8939266048.0000\n",
            "Epoch 105/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 8807086080.0000\n",
            "Epoch 106/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 8652474368.0000\n",
            "Epoch 107/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 8514266624.0000\n",
            "Epoch 108/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 8368534016.0000\n",
            "Epoch 109/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 8248114688.0000\n",
            "Epoch 110/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 8116253696.0000\n",
            "Epoch 111/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 7984048640.0000\n",
            "Epoch 112/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 7867595776.0000\n",
            "Epoch 113/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 7750210560.0000\n",
            "Epoch 114/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 7636119552.0000\n",
            "Epoch 115/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 7530941952.0000\n",
            "Epoch 116/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 7429627392.0000\n",
            "Epoch 117/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 7323190272.0000\n",
            "Epoch 118/200\n",
            "157/157 [==============================] - 0s 1ms/step - loss: 7229895680.0000\n",
            "Epoch 119/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 7133508608.0000\n",
            "Epoch 120/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 7053634560.0000\n",
            "Epoch 121/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6965235200.0000\n",
            "Epoch 122/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6893574656.0000\n",
            "Epoch 123/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6801630208.0000\n",
            "Epoch 124/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6727668736.0000\n",
            "Epoch 125/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6677796352.0000\n",
            "Epoch 126/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6598221824.0000\n",
            "Epoch 127/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6532590592.0000\n",
            "Epoch 128/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6475320832.0000\n",
            "Epoch 129/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6413885952.0000\n",
            "Epoch 130/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6349053440.0000\n",
            "Epoch 131/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6308446720.0000\n",
            "Epoch 132/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6248610304.0000\n",
            "Epoch 133/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6200279040.0000\n",
            "Epoch 134/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6154467328.0000\n",
            "Epoch 135/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6125744640.0000\n",
            "Epoch 136/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6073854976.0000\n",
            "Epoch 137/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6048892416.0000\n",
            "Epoch 138/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 6005920256.0000\n",
            "Epoch 139/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5972224000.0000\n",
            "Epoch 140/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5933728256.0000\n",
            "Epoch 141/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5912100352.0000\n",
            "Epoch 142/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5870105088.0000\n",
            "Epoch 143/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5844491264.0000\n",
            "Epoch 144/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5818237440.0000\n",
            "Epoch 145/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5791261696.0000\n",
            "Epoch 146/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5775962624.0000\n",
            "Epoch 147/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5755458048.0000\n",
            "Epoch 148/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5730882048.0000\n",
            "Epoch 149/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5707944960.0000\n",
            "Epoch 150/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5702518784.0000\n",
            "Epoch 151/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5675544064.0000\n",
            "Epoch 152/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5653835264.0000\n",
            "Epoch 153/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5641122816.0000\n",
            "Epoch 154/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5638969344.0000\n",
            "Epoch 155/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5613839872.0000\n",
            "Epoch 156/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5605515264.0000\n",
            "Epoch 157/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5583769600.0000\n",
            "Epoch 158/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5570553856.0000\n",
            "Epoch 159/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5558445056.0000\n",
            "Epoch 160/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5552570880.0000\n",
            "Epoch 161/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5537949184.0000\n",
            "Epoch 162/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5538532352.0000\n",
            "Epoch 163/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5528418816.0000\n",
            "Epoch 164/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5513585664.0000\n",
            "Epoch 165/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5498548736.0000\n",
            "Epoch 166/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5495329792.0000\n",
            "Epoch 167/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5495047680.0000\n",
            "Epoch 168/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5479375360.0000\n",
            "Epoch 169/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5474003968.0000\n",
            "Epoch 170/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5467971072.0000\n",
            "Epoch 171/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5457476096.0000\n",
            "Epoch 172/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5453433856.0000\n",
            "Epoch 173/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5446192128.0000\n",
            "Epoch 174/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5447552000.0000\n",
            "Epoch 175/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5440212480.0000\n",
            "Epoch 176/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5427812352.0000\n",
            "Epoch 177/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5427423744.0000\n",
            "Epoch 178/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5417525248.0000\n",
            "Epoch 179/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5416746496.0000\n",
            "Epoch 180/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5420450816.0000\n",
            "Epoch 181/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5404605952.0000\n",
            "Epoch 182/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5398830080.0000\n",
            "Epoch 183/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5396346368.0000\n",
            "Epoch 184/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5395975680.0000\n",
            "Epoch 185/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5393044480.0000\n",
            "Epoch 186/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5384603648.0000\n",
            "Epoch 187/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5383864832.0000\n",
            "Epoch 188/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5379173376.0000\n",
            "Epoch 189/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5381730304.0000\n",
            "Epoch 190/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5374617600.0000\n",
            "Epoch 191/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5368262656.0000\n",
            "Epoch 192/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5367020544.0000\n",
            "Epoch 193/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5362666496.0000\n",
            "Epoch 194/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5368698880.0000\n",
            "Epoch 195/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5357575680.0000\n",
            "Epoch 196/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5361340416.0000\n",
            "Epoch 197/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5356123136.0000\n",
            "Epoch 198/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5347728384.0000\n",
            "Epoch 199/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5344250880.0000\n",
            "Epoch 200/200\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 5337711616.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f82fcee25d0>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Generate Predictions\n",
        "\n",
        "* Generate 100 predictions per week with a given scenario."
      ],
      "metadata": {
        "id": "A5-iHVXGCo5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preds(year,pre_covid=False,expansion=True):\n",
        "  '''\n",
        "  Generate 100 predictions with the given features for a specified year.\n",
        "  '''\n",
        "  predictions = pd.DataFrame(columns=['Week', 'Prediction'])\n",
        "\n",
        "  if year > 2021:\n",
        "    inc = year - 2022\n",
        "  else:\n",
        "    inc = 0\n",
        "  \n",
        "  for i in range(0,100):\n",
        "    test_data = get_test_data(year,pre_covid,expansion)\n",
        "    test_data = scaler.transform(test_data)\n",
        "    test_df = pd.DataFrame()\n",
        "    test_df['Week'] = range(0 + inc * 52,52 + inc * 52)\n",
        "    test_df['Prediction'] = model.predict(test_data)\n",
        "    predictions = pd.concat([predictions, test_df],ignore_index=True)\n",
        "\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "4QagiZT_629l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcwe_2022 = get_preds(2022)\n",
        "pcwe_2023 = get_preds(2023)\n",
        "pcwe_2024 = get_preds(2024)\n",
        "pcne_2022 = get_preds(2022,expansion=False)\n",
        "pcne_2023 = get_preds(2023,expansion=False)\n",
        "pcne_2024 = get_preds(2024,expansion=False)\n",
        "ncwe_2022 = get_preds(2022,pre_covid=True)\n",
        "ncwe_2023 = get_preds(2023,pre_covid=True)\n",
        "ncwe_2024 = get_preds(2024,pre_covid=True)\n",
        "ncne_2022 = get_preds(2022,pre_covid=True,expansion=False)\n",
        "ncne_2023 = get_preds(2023,pre_covid=True,expansion=False)\n",
        "ncne_2024 = get_preds(2024,pre_covid=True,expansion=False)\n",
        "\n",
        "pcwe = pd.concat([pcwe_2022,pcwe_2023,pcwe_2024],ignore_index=True)\n",
        "pcne = pd.concat([pcne_2022,pcne_2023,pcne_2024],ignore_index=True)\n",
        "ncwe = pd.concat([ncwe_2022,ncwe_2023,ncwe_2024],ignore_index=True)\n",
        "ncne = pd.concat([ncne_2022,ncne_2023,ncne_2024],ignore_index=True)"
      ],
      "metadata": {
        "id": "zSZ3P4PAJf2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = pd.concat([pcwe,pcne['Prediction'],ncwe['Prediction'],ncne['Prediction']],axis=1,ignore_index=True)\n",
        "predictions.columns = ['Week','Post-Covid w/ Expansion','Post-Covid no Expansion',\n",
        "                       'Pre-Covid w/ Expansion','Pre-Covid no Expansion']"
      ],
      "metadata": {
        "id": "P52wytFEslou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions['Week'] = predictions['Week'].apply(lambda x: x + 156)"
      ],
      "metadata": {
        "id": "iK59HAjy620f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_2022 = pd.DataFrame([248779, 384142, 396384, 439561, 452715, 461370, 461929, 356563])\n",
        "true_data = pd.concat([model_data['Total Boardings'], true_2022],ignore_index=True)\n",
        "true_data.columns = ['Total Boardings']"
      ],
      "metadata": {
        "id": "1tuvoviJ7zaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Results"
      ],
      "metadata": {
        "id": "JWvVX6GGC58A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "2v2k8kmfUBJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(predictions,\n",
        "                 template='none',\n",
        "                 x='Week',\n",
        "                 y=['Week','Post-Covid w/ Expansion','Post-Covid no Expansion','Pre-Covid w/ Expansion',\n",
        "                    'Pre-Covid no Expansion'],\n",
        "                trendline='lowess',trendline_options=dict(frac=.2),opacity=0,title='Total Boardings Predictions',\n",
        "                labels={\n",
        "                          'Week' : 'Year',\n",
        "                          'value' : 'Predicted Weekly Boardings',\n",
        "                          'variable' : 'Scenario'\n",
        "                      })\n",
        "fig.update_xaxes(\n",
        "    ticktext=['2019', '2020', '2021', '2022', '2023', '2024'],\n",
        "    tickvals=[51, 103, 155, 207, 259, 312],\n",
        "  )\n",
        "fig.data = [t for t in fig.data if t.mode == 'lines']\n",
        "fig.update_traces(showlegend=True, selector=dict(mode='lines'))\n",
        "fig.add_traces(go.Scatter(name='True Data',x=true_data.index.values,y=true_data['Total Boardings']))\n",
        "fig.update_traces(hovertemplate='Predicted Boardings: %{y}')\n",
        "fig.add_traces(go.Scatter(name='Post-Covid w/ Expansion',showlegend=False,line=dict(color='lightblue',width=0),\n",
        "                          x=np.concatenate([pcwe.groupby('Week').describe()['Prediction'].index.values,\n",
        "                                            pcwe.groupby('Week').describe()['Prediction'].index.values[::-1]]) + 156,\n",
        "                          y=pd.concat([pcwe.groupby('Week').describe()['Prediction']['75%'],\n",
        "                                       pcwe.groupby('Week').describe()['Prediction']['25%'][::-1]]), fill='toself'))\n",
        "fig.add_traces(go.Scatter(name='Post-Covid no Expansion',showlegend=False,line=dict(color='salmon',width=0),\n",
        "                          x=np.concatenate([pcne.groupby('Week').describe()['Prediction'].index.values,\n",
        "                                            pcne.groupby('Week').describe()['Prediction'].index.values[::-1]]) + 156,\n",
        "                          y=pd.concat([pcne.groupby('Week').describe()['Prediction']['75%'],\n",
        "                                       pcne.groupby('Week').describe()['Prediction']['25%'][::-1]]), fill='toself'))\n",
        "fig.add_traces(go.Scatter(name='Pre-Covid w/ Expansion',showlegend=False,line=dict(color='lightgreen',width=0),\n",
        "                          x=np.concatenate([ncwe.groupby('Week').describe()['Prediction'].index.values,\n",
        "                                            ncwe.groupby('Week').describe()['Prediction'].index.values[::-1]]) + 156,\n",
        "                          y=pd.concat([ncwe.groupby('Week').describe()['Prediction']['75%'],\n",
        "                                       ncwe.groupby('Week').describe()['Prediction']['25%'][::-1]]), fill='toself'))\n",
        "fig.add_traces(go.Scatter(name='Pre-Covid no Expansion',showlegend=False,line=dict(color='mediumvioletred',width=0),\n",
        "                          x=np.concatenate([ncne.groupby('Week').describe()['Prediction'].index.values,\n",
        "                                            ncne.groupby('Week').describe()['Prediction'].index.values[::-1]]) + 156,\n",
        "                          y=pd.concat([ncne.groupby('Week').describe()['Prediction']['75%'],\n",
        "                                       ncne.groupby('Week').describe()['Prediction']['25%'][::-1]]), fill='toself'))\n",
        "fig.data = fig.data[::-1]\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "U7VaA-r-6Mg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ICiPyiIWALo2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}